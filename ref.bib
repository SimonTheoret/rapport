@Comment TODO: Retirer références inutiles
@misc{soup,
    title = {Model soups: averaging weights of multiple fine-tuned models
             improves accuracy without increasing inference time},
    author = {Mitchell Wortsman and Gabriel Ilharco and Samir Yitzhak Gadre and
              Rebecca Roelofs and Raphael Gontijo-Lopes and Ari S. Morcos and
              Hongseok Namkoong and Ali Farhadi and Yair Carmon and Simon
              Kornblith and Ludwig Schmidt},
    year = {2022},
    eprint = {2203.05482},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
    url = {https://arxiv.org/abs/2203.05482},
}

@misc{clark2020electrapretrainingtextencoders,
    title = {ELECTRA: Pre-training Text Encoders as Discriminators Rather Than
             Generators},
    author = {Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D.
              Manning},
    year = {2020},
    eprint = {2003.10555},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
    url = {https://arxiv.org/abs/2003.10555},
}

@misc{schemas,
    title = {Text Chunking using Transformation-Based Learning},
    author = {Lance A. Ramshaw and Mitchell P. Marcus},
    year = {1995},
    eprint = {cmp-lg/9505040},
    archivePrefix = {arXiv},
    primaryClass = {cmp-lg},
    url = {https://arxiv.org/abs/cmp-lg/9505040},
}

@book{Corduneanu,
    author = {Corduneanu, C.},
    title = {Integral Equations and Stability of Feedback Systems},
    publisher = {Academic Press, New York},
    year = {1973},
}

@article{mawhin1987,
    author = {Mawhin, Jean},
    title = {First order ordinary differential equations with several periodic
             solutions},
    journal = {Zeitschrift f{\"u}r angewandte Mathematik und Physik ZAMP},
    year = {1987},
    volume = {38},
    number = {2},
    pages = {257--265},
    publisher = {Springer},
}

@incollection{Milnor,
    author = {J Milnor},
    title = {Morse Theory},
    booktitle = {Annals of mathematics studies},
    year = {1963},
    volumes = {51},
    publisher = {ABC},
}

@misc{onnxruntime,
    title = {ONNX Runtime},
    author = {ONNX Runtime developers},
    year = {2021},
    howpublished = {\url{https://onnxruntime.ai/}},
    note = {Version: x.y.z},
}

@misc{vaswani2023attentionneed,
    title = {Attention Is All You Need},
    author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob
              Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and
              Illia Polosukhin},
    year = {2023},
    eprint = {1706.03762},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
    url = {https://arxiv.org/abs/1706.03762},
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond
              and Clement Delangue and Anthony Moi and Pierric Cistac and Tim
              Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam
              Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and
              Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and
              Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in
                 Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45",
}

@misc{bert,
    title = {BERT: Pre-training of Deep Bidirectional Transformers for Language
             Understanding},
    author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina
              Toutanova},
    year = {2019},
    eprint = {1810.04805},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
    url = {https://arxiv.org/abs/1810.04805},
}

@misc{watanabe2023treestructuredparzenestimatorunderstanding,
    title = {Tree-Structured Parzen Estimator: Understanding Its Algorithm
             Components and Their Roles for Better Empirical Performance},
    author = {Shuhei Watanabe},
    year = {2023},
    eprint = {2304.11127},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG},
    url = {https://arxiv.org/abs/2304.11127},
}

@inproceedings{NIPS2011_86e8f7ab,
    author = {Bergstra, James and Bardenet, R\'{e}mi and Bengio, Yoshua and K\'{
              e}gl, Bal\'{a}zs},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and
              K.Q. Weinberger},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Algorithms for Hyper-Parameter Optimization},
    url = {
           https://proceedings.neurips.cc/paper_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf
           },
    volume = {24},
    year = {2011},
}
@misc{penedo2024datatrove,
    author = {Penedo, Guilherme and Kydlíček, Hynek and Cappelli, Alessandro and
              Sasko, Mario and Wolf, Thomas},
    title = {DataTrove: large scale data processing},
    year = {2024},
    publisher = {GitHub},
    journal = {GitHub repository},
    url = {https://github.com/huggingface/datatrove},
}
@article{joulin2016fasttext,
    title = {FastText.zip: Compressing text classification models},
    author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze,
              Matthijs and J{\'e}gou, H{\'e}rve and Mikolov, Tomas},
    journal = {arXiv preprint arXiv:1612.03651},
    year = {2016},
}

@misc{wu2016googlesneuralmachinetranslation,
    title = {Google's Neural Machine Translation System: Bridging the Gap
             between Human and Machine Translation},
    author = {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and
              Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan
              Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva
              Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and
              Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa
              and Keith Stevens and George Kurian and Nishant Patil and Wei Wang
              and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick
              and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey
              Dean},
    year = {2016},
    eprint = {1609.08144},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL},
    url = {https://arxiv.org/abs/1609.08144},
}

@inproceedings{oscar,
  author    = {Pedro Javier {Ortiz Su{\'a}rez} and Beno{\^i}t Sagot and Laurent Romary},
  title     = {Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures},
  series = {Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019},
  editor    = {Piotr Bański and Adrien Barbaresi and Hanno Biber and Evelyn Breiteneder and Simon Clematide and Marc Kupietz and Harald L{\"u}ngen and Caroline Iliadi},
  publisher = {Leibniz-Institut f{\"u}r Deutsche Sprache},
  address   = {Mannheim},
  doi       = {10.14618/ids-pub-9021},
  url       = {http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215},
  pages     = {9 -- 16},
  year      = {2019},
  abstract  = {Common Crawl is a considerably large, heterogeneous multilingual corpus comprised of crawled documents from the internet, surpassing 20TB of data and distributed as a set of more than 50 thousand plain text files where each contains many documents written in a wide variety of languages. Even though each document has a metadata block associated to it, this data lacks any information about the language in which each document is written, making it extremely difficult to use Common Crawl for monolingual applications. We propose a general, highly parallel, multithreaded pipeline to clean and classify Common Crawl by language; we specifically design it so that it runs efficiently on medium to low resource infrastructures where I/O speeds are the main constraint. We develop the pipeline so that it can be easily reapplied to any kind of heterogeneous corpus and so that it can be parameterised to a wide range of infrastructures. We also distribute a 6.3TB version of Common Crawl, filtered, classified by language, shuffled at line level in order to avoid copyright issues, and ready to be used for NLP applications.},
  language  = {en}
}

@inproceedings{camembert,
   title={CamemBERT: a Tasty French Language Model},
   url={http://dx.doi.org/10.18653/v1/2020.acl-main.645},
   DOI={10.18653/v1/2020.acl-main.645},
   booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
   publisher={Association for Computational Linguistics},
   author={Martin, Louis and Muller, Benjamin and Ortiz Suárez, Pedro Javier and Dupont, Yoann and Romary, Laurent and de la Clergerie, Éric and Seddah, Djamé and Sagot, Benoît},
   year={2020} }
